if __name__ == "__main__":
    import sys
    import os
    import pathlib

    ROOT_DIR = str(pathlib.Path(__file__).parent.absolute())
    sys.path.append(ROOT_DIR)

import hydra
import torch
import zarr
import numpy as np
import time
from tqdm import tqdm
from omegaconf import OmegaConf
from diffusion_policy_3d.workspace.balancer_workspace import BalancerWorkspace
from diffusion_policy_3d.common.pytorch_util import dict_apply

# ==========================================
# GPU K-Means å®ç°
# ==========================================
def kmeans_pytorch(X, k=100, n_iter=30, batch_size=50000, tol=1e-4, verbose=True):
    """
    X: (N, d) Tensor on GPU
    """
    device = X.device
    N, d = X.shape
    
    # éšæœºåˆå§‹åŒ–ä¸­å¿ƒ
    idx = torch.randperm(N, device=device)[:k]
    centers = X[idx].clone()

    for it in range(n_iter):
        t0 = time.time()
        new_centers_sum = torch.zeros((k, d), device=device)
        new_centers_count = torch.zeros(k, device=device)

        # åˆ†æ‰¹è®¡ç®—è·ç¦»ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
        for i in range(0, N, batch_size):
            xb = X[i:i + batch_size]
            dist = torch.cdist(xb, centers)
            labels = torch.argmin(dist, dim=1)
            
            for c in range(k):
                mask = (labels == c)
                if mask.any():
                    new_centers_sum[c] += xb[mask].sum(dim=0)
                    new_centers_count[c] += mask.sum()

        # æ›´æ–°ä¸­å¿ƒ
        updated = new_centers_sum / new_centers_count.unsqueeze(1).clamp(min=1)
        shift = torch.norm(centers - updated, dim=1).mean()
        centers = updated

        if verbose:
            print(f"[Iter {it+1}] shift={shift:.6f}, time={time.time()-t0:.3f}s")
        if shift < tol:
            break

    # æœ€åå†ç®—ä¸€æ¬¡æ‰€æœ‰æ•°æ®çš„ label
    final_labels = torch.empty(N, dtype=torch.long, device=device)
    for i in range(0, N, batch_size):
        xb = X[i:i + batch_size]
        dist = torch.cdist(xb, centers)
        final_labels[i:i + batch_size] = torch.argmin(dist, dim=1)

    cluster_density = torch.bincount(final_labels, minlength=k).float()
    return centers, final_labels, cluster_density



import hashlib
def batch_feature_hash(features: torch.Tensor) -> tuple[list[str], dict[str, int]]:
    """
    å¯¹ encoder æå–åçš„æ‰¹é‡ç‰¹å¾å‘é‡ç”Ÿæˆå”¯ä¸€å“ˆå¸Œå€¼ + å“ˆå¸Œ-æ ·æœ¬ç´¢å¼•æ˜ å°„
    é€‚é…å½¢çŠ¶ï¼š[N, Feature_Dim]ï¼ˆNä¸ºæ€»æ ·æœ¬æ•°ï¼ŒFeature_Dimä¸ºencoderè¾“å‡ºç‰¹å¾ç»´åº¦ï¼‰
    
    Args:
        features: encoder æå–åçš„ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ [N, Feature_Dim]ï¼ˆæ”¯æŒCPU/GPUå¼ é‡ï¼‰
        
    Returns:
        hash_list: é•¿åº¦ä¸ºNçš„å“ˆå¸Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œé¡ºåºä¸è¾“å…¥ç‰¹å¾ä¸€è‡´
        hash_to_idx: å“ˆå¸Œå€¼â†’æ ·æœ¬ç´¢å¼•çš„æ˜ å°„å­—å…¸ï¼ˆç¡®ä¿å“ˆå¸Œå”¯ä¸€ï¼‰
    """
    # 1. ç‰¹å¾é¢„å¤„ç†ï¼šè„±ç¦»è®¡ç®—å›¾ + è½¬CPU + è½¬numpyï¼ˆå…¼å®¹GPUå¼ é‡ï¼Œç¡®ä¿å“ˆå¸Œç¡®å®šæ€§ï¼‰
    features_np = features.detach().cpu().numpy()
    
    # 2. éå†æ¯ä¸ªæ ·æœ¬ç”Ÿæˆå“ˆå¸Œï¼Œæ„å»ºæ˜ å°„
    hash_list = []
    hash_to_idx = {}
    for idx, vec in enumerate(features_np):
        # å‘é‡è½¬å­—èŠ‚æµï¼ˆæ•°å€¼å®Œå…¨ä¸€è‡´åˆ™å­—èŠ‚æµä¸€è‡´ï¼Œä¿è¯å“ˆå¸Œç¡®å®šæ€§ï¼‰
        vec_bytes = vec.tobytes()
        # SHA256ç”Ÿæˆ64ä½å”¯ä¸€å“ˆå¸Œ
        hash_str = hashlib.sha256(vec_bytes).hexdigest()
        
        # ç¢°æ’æ£€æµ‹ï¼šç¡®ä¿å“ˆå¸Œå”¯ä¸€ï¼ˆå‡ºç°ç¢°æ’ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…åç»­é—®é¢˜ï¼‰
        if hash_str in hash_to_idx:
            raise ValueError(f"å“ˆå¸Œç¢°æ’ï¼æ ·æœ¬ {idx} ä¸æ ·æœ¬ {hash_to_idx[hash_str]} ç”Ÿæˆç›¸åŒå“ˆå¸Œï¼š{hash_str}")
        
        hash_list.append(hash_str)
        hash_to_idx[hash_str] = idx  # æ˜ å°„åˆ°åŸå§‹æ ·æœ¬ç´¢å¼•ï¼ˆå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹ä¸ºæ˜ å°„åˆ°ç‰¹å¾/èšç±»æ ‡ç­¾ï¼‰
    
    return hash_list, hash_to_idx


# ==========================================
# æ ¸å¿ƒé€»è¾‘
# ==========================================
@hydra.main(
    config_path="/home/shui/idp3_test/Improved-3D-Diffusion-Policy/Improved-3D-Diffusion-Policy/diffusion_policy_3d/config", 
    config_name="controlnet.yaml", 
    version_base=None
)
def main(cfg):
    device = torch.device(cfg.training.device)
    
    # 1. å®ä¾‹åŒ– BalancerWorkspace
    print(f"ğŸ”„ Instantiating BalancerWorkspace...")
    workspace = BalancerWorkspace(cfg)
    
    # 2. åŠ è½½ Checkpoint
    # è¯·ç¡®è®¤è¿™ä¸ªè·¯å¾„æ˜¯å¦æ­£ç¡®
    ckpt_path = pathlib.Path("/home/shui/idp3_test/Improved-3D-Diffusion-Policy/Improved-3D-Diffusion-Policy/data/outputs/-controlnet-DMP_dualarm_augment_stage1_12011325_seed0/checkpoints/latest.ckpt")    
    print(f"ğŸ“¥ Loading checkpoint from: {ckpt_path}")
    if not ckpt_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")
    
    workspace.load_checkpoint(path=ckpt_path)
    
    # 3. æå– Policy å’Œ Normalizer
    # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ ema_model è¿˜æ˜¯ model
    if cfg.stage1.training.use_ema and workspace.ema_model is not None:
        policy = workspace.ema_model
        print("âœ… Using EMA Model")
    else:
        policy = workspace.model
        print("âœ… Using Standard Model")
        
    policy.eval()
    policy.to(device)
    
    # è·å– Encoder
    encoder = policy.obs_encoder_stage1
    # è·å– Normalizer (è‡³å…³é‡è¦)
    normalizer = policy.normalizer
    
    print("ğŸ” Encoder found:", type(encoder))

# ... å‰é¢çš„ä»£ç ä¸å˜ ...

    # 4. æ‰“å¼€ Zarr æ•°æ®
    zarr_path = "/home/shui/idp3_test/Improved-3D-Diffusion-Policy/data/dataset_1130_original.zarr"
    print(f"ğŸ“‚ Opening Zarr: {zarr_path}")
    root = zarr.open(zarr_path, mode='r')
    
    # [ä¿®æ”¹ç‚¹ 1] é¢„å…ˆè·å–æ‰€æœ‰éœ€è¦çš„ Array
    pc_array = root['data']['point_cloud']
    
    # å°è¯•è·å– state å’Œ action
    state_array = root['data']['state'] if 'state' in root['data'] else None
    action_array = root['data']['action'] if 'action' in root['data'] else None
    
    total_len = pc_array.shape[0]
    print(f"ğŸ“Š Total frames: {total_len}")
    if state_array is not None: print(f"   State shape: {state_array.shape}")
    if action_array is not None: print(f"   Action shape: {action_array.shape}")

    # 5. æ¨ç† Loop
    batch_size = 64
    embeddings_list = []
    
    print("ğŸš€ Starting Encoding...")
    with torch.no_grad():
        for i in tqdm(range(0, total_len, batch_size)):
            # --- A. è¯»å–æ•°æ® ---
            # 1. Point Cloud
            batch_pc = torch.from_numpy(pc_array[i : i + batch_size]).float().to(device)
            
            # 2. State (å¦‚æœæœ‰) -> æ‹†åˆ†ä¸º agent_pos / agent_rot
            # [å…³é”®] è¿™é‡Œéœ€è¦ä½ ç¡®è®¤ dataset çš„ state æ˜¯æ€ä¹ˆå®šä¹‰çš„
            # å‡è®¾: state (N, 14) -> pos (N, 10) + rot (N, 4) æˆ–è€…å…¶ä»–åˆ‡åˆ†æ–¹å¼
            # ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œè¯·æ ¹æ®ä½ çš„ shape_meta ä¿®æ”¹åˆ‡ç‰‡ç´¢å¼•
            if state_array is not None:
                batch_state = torch.from_numpy(state_array[i : i + batch_size]).float().to(device)
                
                # ç¤ºä¾‹åˆ‡åˆ†ï¼šå‡è®¾å‰3ç»´æ˜¯ posï¼Œå6ç»´æ˜¯ rot (æ ¹æ®ä½ çš„ shape_meta è°ƒæ•´)
                # å¦‚æœä½ çš„ normalizer é‡Œé¢æœ‰ agent_pos å’Œ agent_rot çš„ç»Ÿè®¡ä¿¡æ¯
                # ä½ å¿…é¡»ç¡®ä¿è¿™é‡Œçš„æ•°æ®ç»´åº¦å’Œ key èƒ½å¯¹ä¸Š
                agent_pos = batch_state[:, :]  # [è¯·ä¿®æ”¹è¿™é‡Œ]
                # agent_rot = batch_state[:, 3:9] # [è¯·ä¿®æ”¹è¿™é‡Œ]
                
                # å¦‚æœ state ç»´åº¦ä¸å¤Ÿï¼Œæˆ–è€…å¯¹åº”ä¸ä¸Šï¼Œä½ å¯ä»¥ç”¨å…¨0è¡¥å…¨å‰©ä½™ç»´åº¦
                # agent_pos = torch.cat([agent_pos, torch.zeros(...)], dim=-1)
            else:
                # å¦‚æœ zarr é‡Œæ²¡ stateï¼Œåªèƒ½ç”¨ dummy
                agent_pos = None 
                agent_rot = None


            # --- B. æ„é€  Obs Dict ---
            obs_dict = {
                'point_cloud': batch_pc,
            }
            
            # å°†è¯»å–åˆ°çš„ state å¡è¿›å»
            if agent_pos is not None: obs_dict['agent_pos'] = agent_pos
            # if agent_rot is not None: obs_dict['agent_rot'] = agent_rot

            # --- C. å…œåº•é€»è¾‘ (Dummy) ---
            # å¦‚æœ Normalizer æœŸå¾…æŸäº› key (å¦‚ agent_pos)ï¼Œä½† Zarr é‡Œæ²¡æœ‰æˆ–è€…æ²¡è¯»åˆ°
            # å¿…é¡»å¡«è¡¥ Dummy æ•°æ®ï¼Œå¦åˆ™ normalizer ä¼šæŠ¥é”™
            for key in normalizer.params_dict.keys():
                if key not in obs_dict and key != 'action':
                    # è·å–è¯¥ key åœ¨ normalizer ä¸­è®°å½•çš„ mean çš„å½¢çŠ¶
                    # shape é€šå¸¸æ˜¯ (1, D)
                    param_shape = normalizer.params_dict[key]['mean'].shape
                    # æ„é€  (B, D) çš„å…¨0æ•°æ®
                    dummy = torch.zeros((batch_pc.shape[0], *param_shape[1:]), device=device)
                    obs_dict[key] = dummy

            # --- D. å½’ä¸€åŒ– ---
            # normalizer.normalize(dict) åªä¼šå¤„ç† dict ä¸­å­˜åœ¨çš„ key
            # å¹¶ä¸”ä¼šå¿½ç•¥ 'action' (å› ä¸º action é€šå¸¸åœ¨ normalizer['action'] é‡Œå•ç‹¬å¤„ç†)
            nobs = normalizer.normalize(obs_dict)
            
            # å¤„ç†é¢œè‰²
            if hasattr(policy, 'use_pc_color') and policy.use_pc_color:
                nobs['point_cloud'][..., 3:] /= 1.0
            else:
                nobs['point_cloud'] = nobs['point_cloud'][..., :]

            # --- E. Encoder æ¨ç† ---
            encoded = encoder(nobs)
            
            if len(encoded.shape) > 2:
                encoded = encoded.reshape(encoded.shape[0], -1)
            embeddings_list.append(encoded)


    # åˆå¹¶æ‰€æœ‰ batch
    X = torch.cat(embeddings_list, dim=0)
    print(f"ğŸ” First 2 embeddings: {X[:2]}")
    print(f"âœ… Features extracted. Shape: {X.shape}")
    
    
    print("\nğŸ”¨ å¼€å§‹å¯¹ encoder ç‰¹å¾ç”Ÿæˆå“ˆå¸Œ...")
    # hash_list, hash_to_idx = batch_feature_hash(X)
    encode_feature = nn.randint(64, 512)
    hash_list, hash_to_idx = batch_feature_hash(encode_feature)
    
    
    # è¾“å‡ºå“ˆå¸Œç»“æœç»Ÿè®¡
    print(f"âœ… å“ˆå¸Œç”Ÿæˆå®Œæˆï¼")
    print(f"   æ€»å“ˆå¸Œæ•°ï¼š{len(hash_list)}ï¼ˆä¸æ ·æœ¬æ•°ä¸€è‡´ï¼‰")
    print(f"   å“ˆå¸Œå­—ç¬¦ä¸²é•¿åº¦ï¼š{len(hash_list[0])}ï¼ˆSHA256æ ‡å‡†64ä½åå…­è¿›åˆ¶ï¼‰")
    print(f"   å“ˆå¸Œæ˜ å°„å­—å…¸å¤§å°ï¼š{len(hash_to_idx)}ï¼ˆæ— ç¢°æ’ï¼Œä¸æ ·æœ¬æ•°ä¸€è‡´ï¼‰")
    
    # æ‰“å°å‰5ä¸ªæ ·æœ¬çš„å“ˆå¸Œç¤ºä¾‹
    print("\nğŸ“‹ å‰5ä¸ªæ ·æœ¬çš„å“ˆå¸Œç¤ºä¾‹ï¼š")
    for idx in range(5):
        print(f"   æ ·æœ¬ {idx} â†’ å“ˆå¸Œå€¼ï¼š{hash_list[idx]}")
    
    # éªŒè¯å“ˆå¸Œå”¯ä¸€æ€§ï¼ˆå¯é€‰ï¼Œå¿«é€Ÿæ ¡éªŒï¼‰
    sample_idx1, sample_idx2 = 0, 100  # ä»»æ„ä¸¤ä¸ªä¸åŒæ ·æœ¬
    print(f"\nâœ… å”¯ä¸€æ€§éªŒè¯ï¼š")
    print(f"   æ ·æœ¬ {sample_idx1} ä¸æ ·æœ¬ {sample_idx2} å“ˆå¸Œæ˜¯å¦ä¸åŒï¼Ÿ{hash_list[sample_idx1] != hash_list[sample_idx2]}")

    # ï¼ˆå¯é€‰ï¼‰ä¿å­˜å“ˆå¸Œç»“æœåˆ°æ–‡ä»¶ï¼ˆæ–¹ä¾¿åç»­KNNå¯†åº¦è®¡ç®—ã€æ ·æœ¬å…³è”ï¼‰
    # np.savez('feature_hash_results.npz', hash_list=hash_list, hash_to_idx=hash_to_idx)
    # print(f"\nğŸ’¾ å“ˆå¸Œç»“æœå·²ä¿å­˜åˆ° feature_hash_results.npz")



if __name__ == "__main__":
    main()